The Array: The Foundational Data Structure
Most data structures are used in four basic ways, which we refer to as operations.
The Array: The Foundational Data Structure
Read:
The Array: The Foundational Data Structure
Search:
The Array: The Foundational Data Structure
Insert:
The Array: The Foundational Data Structure
Delete:
The Array: The Foundational Data Structure
when we measure how “fast” an operation takes, we do not refer to how fast the operation takes in terms of pure time, but instead in how many steps it takes.
The Array: The Foundational Data Structure
Measuring the speed of an operation is also known as measuring its time complexity. Throughout this book, we’ll use the terms speed, time complexity, efficiency, and performance interchangeably. They all refer to the number of steps that a given operation takes.
Reading
Reading from an array actually takes just one step.
Reading
When a program declares an array, it allocates a contiguous set of empty cells for use in the program.
Reading
Now, every cell in a computer’s memory has a specific address.
Reading
Each cell’s memory address is one number greater than the previous cell
Reading
Reading from an array is, therefore, a very efficient operation, since it takes just one step. An operation with just one step is naturally the fastest type of operation.
Searching
As we stated previously, searching an array is looking to see whether a particular value exists within an array and if so, which index it’s located at
Searching
To search for a value within an array, the computer starts at index 0, checks the value, and if it doesn’t find what it’s looking for, moves on to the next index. It does this until it finds the value it’s seeking.
Searching
Another way of saying this is that for N cells in an array, linear search will take a maximum of N steps. In this context, N is just a variable that can be replaced by any number.
Insertion
The efficiency of inserting a new piece of data inside an array depends on where inside the array you’d like to insert it.
Insertion
Let’s say we wanted to add "figs" to the very end of our shopping list. Such an insertion takes just one step
Insertion
Inserting a new piece of data at the beginning or the middle of an array, however, is a different story. In these cases, we need to shift many pieces of
Insertion
data to make room for what we’re inserting, leading to additional steps.
Insertion
The worst-case scenario for insertion into an array—that is, the scenario in which insertion takes the most steps—is where we insert data at the beginning of the array. This is because when inserting into the beginning of the array, we have to move all the other values one cell to the right.
Insertion
So we can say that insertion in a worst-case scenario can take up to N + 1 steps for an array containing N elements. This is because the worst-case scenario is inserting a value into the beginning of the array in which there are N shifts (every data element of the array) and one insertion.
Deletion
While the actual deletion of "cucumbers" technically took just one step, we now have a problem: we have an empty cell sitting smack in the middle of our array. An array is not allowed to have gaps in the middle of it, so to resolve this issue, we need to shift "dates" and "elderberries" to the left.
Deletion
We can conclude, then, that for an array containing N elements, the maximum number of steps that deletion would take is N steps
Sets: How a Single Rule Can Affect Efficiency
A set is a data structure that does not allow duplicate values to be contained within it.
Sets: How a Single Rule Can Affect Efficiency
The only difference between this set and a classic array is that the set never allows duplicate values to be inserted into it.
Sets: How a Single Rule Can Affect Efficiency
In any case, a set is an array with one simple constraint of not allowing duplicates. Yet, this constraint actually causes the set to have a different efficiency for one of the four primary operations.
Sets: How a Single Rule Can Affect Efficiency
Reading from a set is exactly the same as reading from an array—it takes just one step for the computer to look up what is contained within a particular index
Sets: How a Single Rule Can Affect Efficiency
Searching a set also turns out to be no different than searching an array—it takes up to N steps to search to see if a value exists within a set. And deletion is also identical between a set and an array—it takes up to N steps to delete a value and move data to the left to close the gap.
Sets: How a Single Rule Can Affect Efficiency
nsertion, however, is where arrays and sets diverge. Let’s first explore inserting a value at the end of a set, which was a best-case scenario for an array. With an array, the computer can insert a value at its end in a single step. With a set, however, the computer first needs to determine that this value doesn’t already exist in this set—because that’s what sets do: they prevent duplicate data. So every insert first requires a search.
Sets: How a Single Rule Can Affect Efficiency
insertion into a set in a best-case scenario will take N + 1 steps for N elements. This is because there are N steps of search to ensure that the value doesn’t already exist within the set, and then one step for the actual insertion.
Sets: How a Single Rule Can Affect Efficiency
In a worst-case scenario, where we’re inserting a value at the beginning of a set, the computer needs to search N cells to ensure that the set doesn’t already contain that value, and then another N steps to shift all the data to the right, and another final step to insert the new value. That’s a total of 2N + 1 steps.
2. Why Algorithms Matter
An algorithm is simply a particular process for solving a problem.
Ordered Arrays
The ordered array is almost identical to the array we discussed in the previous chapter. The only difference is that ordered arrays require that the values are always kept—you guessed it—in order. That is, every time a value is added, it gets placed in the proper cell so that the values of the array remain sorted
Binary Search
The major advantage of an ordered array over a standard array is that we have the option of performing a binary search rather than a linear search. Binary search is impossible with a standard array because the values can be in any order.
Binary Search vs. Linear Search
The pattern that emerges is that for every time we double the number of items in the ordered array, the number of steps needed for binary search increases by just one.

3. Oh Yes! Big O Notation
Known as Big O Notation, this formalized expression around these concepts allows us to easily categorize the efficiency of a given algorithm and convey it to others.
Big O: Count the Steps
Instead of focusing on units of time, Big O achieves consistency by focusing only on the number of steps that an algorithm takes.
Big O: Count the Steps
reading from an array takes just one step, no matter how large the array is. The way to express this in Big O Notation is: O(1)
Big O: Count the Steps
O(1) simply means that the algorithm takes the same number of steps no matter how much data there is. In this case, reading from an array always takes just one step no matter how much data the array contains
Big O: Count the Steps
Other operations that fall under the category of O(1) are the insertion and deletion of a value at the end of an array. As we’ve seen, each of these operations takes just one step for arrays of any size, so we’d describe their efficiency as O(1).
Big O: Count the Steps
near search is the process of searching an array for a particular value by checking each cell, one at a time. In a worst-case scenario, linear search will take as many steps as there are elements in the array. As we’ve previously phrased it: for N elements in the array, linear search can take up to a maximum of N steps. The appropriate way to express this in Big O Notation is: O(N) I pronounce this as “Oh of N.”
Big O: Count the Steps
Big O is originally a concept from mathematics, and therefore it’s often described in mathematical terms. For example, one way of describing Big O is that it describes the upper bound of the growth rate of a function, or that if a function g(x) grows no faster than a function f(x), then g is said to be a member of O(f).
Constant Time vs. Linear Time
ig O Notation does more than simply describe the number of steps that an algorithm takes, such as a hard number such as 22 or 400. Rather, it describes how many steps an algorithm takes based on the number of data elements that the algorithm is acting upon. Another way of saying this is that Big O answers the following question: how does the number of steps change as the data increases?
Constant Time vs. Linear Time
As Big O is primarily concerned about how an algorithm performs across varying amounts of data, an important point emerges: an algorithm can be described as O(1) even if it takes more than one step. Let’s say that a particular algorithm always takes three steps, rather than one—but it always takes these three steps no matter how much data there is.
Constant Time vs. Linear Time
Because the number of steps remains constant no matter how much data there is, this would also be considered constant time and be described by Big O Notation as O(1). Even though the algorithm technically takes three steps rather than one step, Big O Notation considers that trivial. O(1) is the way to describe any algorithm that doesn’t change its number of steps even when the data increases.
Constant Time vs. Linear Time
O(N) is considered to be, on the whole, less efficient than O(1).
Constant Time vs. Linear Time
The same is true for an O(1) algorithm that always takes one million steps. As the data increases, there will inevitably reach a point where O(N) becomes less efficient than the O(1) algorithm, and will remain so up until an infinite amount of data.
Same Algorithm, Different Scenarios
If we were to describe the efficiency of linear search in its totality, we’d say that linear search is O(1) in a best-case scenario, and O(N) in a worst-case scenario.
Same Algorithm, Different Scenarios
While Big O effectively describes both the best- and worst-case scenarios of a given algorithm, Big O Notation generally refers to worst-case scenario unless specified otherwise. This is why most references will describe linear search as being O(N) even though it can be O(1) in a best-case scenario.
An Algorithm of the Third Kind
Binary search seems to fall somewhere in between O(1) and O(N).
An Algorithm of the Third Kind
In Big O, we describe binary search as having a time complexity of: O(log N)
An Algorithm of the Third Kind
Simply put, O(log N) is the Big O way of describing an algorithm that increases one step each time the data is doubled.

Logarithms are the inverse of exponents
2^3 is the equivalent of: 2 * 2 * 2 which just happens to be 8.
log2 8 is the converse of the above. It means: how many times do you have to multiply 2 by itself to get a result of 8? Since you have to multiply 2 by itself 3 times to get 8, log2 8 = 3
Another way of explaining log2 8 is: if we kept dividing 8 by 2 until we ended up with 1, how many 2s would we have in our equation?
8 / 2 / 2 / 2 = 1 In other words, how many times do we need to divide 8 by 2 until we end up with 1? In this example, it takes us 3 times.
O(log N) means that the algorithm takes as many steps as it takes to keep halving the data elements until we remain with one.

Point to two consecutive items in the array. (Initially, we start at the very beginning of the array and point to its first two items.) Compare the first item with the second one:
Bubble Sort
If the two items are out of order (in other words, the left value is greater than the right value), swap them:
Bubble Sort
(If they already happen to be in the correct order, do nothing for this step.)
Bubble Sort
Move the “pointers” one cell to the right:
Bubble Sort
Repeat steps 1 and 2 until we reach the end of the array or any items that have already been sorted.
Bubble Sort
Repeat steps 1 through 3 until we have a round in which we didn’t have to make any swaps. This means that the array is in order. Each time we repeat steps 1 through 3 is known as a passthrough.
The Efficiency of Bubble Sort
Notice the inefficiency here. As the number of elements increase, the number of steps grows exponentially.
The Efficiency of Bubble Sort
If you look precisely at the growth of steps as N increases, you’ll see that it’s growing by approximately N2.
The Efficiency of Bubble Sort
Therefore, in Big O Notation, we would say that Bubble Sort has an efficiency of O(N2).
The Efficiency of Bubble Sort
Said more officially: in an O(N2) algorithm, for N data elements, there are roughly N2 steps.
The Efficiency of Bubble Sort
One last note: O(N2) is also referred to as quadratic time.
A Quadratic Problem
Based on this, we can conclude that for N elements in the array, our function would perform N2 comparisons. This is because we perform an outer loop that must iterate N times to get through the entire array, and for each iteration, we must iterate another N times with our inner loop. That’s N steps * N steps, which is otherwise known as N2 steps, leaving us with an algorithm of O(N2).
A Quadratic Problem
Unsurprisingly, O(N2) is the efficiency of algorithms in which nested loops are used. When you see a nested loop, O(N2) alarm bells should start going off in your head.
A Linear Solution
This new algorithm appears to make N comparisons for N data elements. This is because there’s only one loop, and it simply iterates for as many elements as there are in the array.


Selection Sort
We check each cell of the array from left to right to determine which value is least. As we move from cell to cell, we keep in a variable the lowest value we’ve encountered so far. (Really, we keep track of its index, but for the purposes of the following diagrams, we’ll just focus on the actual value.) If we encounter a cell that contains a value that is even less than the one in our variable, we replace it so that the variable now points to the new index
Selection Sort
Once we’ve determined which index contains the lowest value, we swap that index with the value we began the passthrough with. This would be index 0 in the first passthrough, index 1 in the second passthrough, and so on and so forth. In the next diagram, we make the swap of the first passthrough: Repeat steps 1 and 2 until all the data is sorted.
The Efficiency of Selection Sort
To put it more generally, we’d say that for N elements, we make (N - 1) + (N - 2) + (N - 3) … + 1 comparisons.
The Efficiency of Selection Sort
As for swaps, however, we only need to make a maximum of one swap per passthrough.
Ignoring Constants
But here’s the funny thing: in the world of Big O Notation, Selection Sort and Bubble Sort are described in exactly the same way.
Ignoring Constants
In reality, however, Selection Sort is described in Big O as O(N2), just like Bubble Sort. This is because of a major rule of Big O that we’re now introducing for the first time: Big O Notation ignores constants.
Ignoring Constants
This is simply a mathematical way of saying that Big O Notation never includes regular numbers that aren’t an exponent.
Ignoring Constants
In our case, what should technically be O(N2 / 2) becomes simply O(N2). Similarly, O(2N) would become O(N), and O(N / 2) would also become O(N). Even O(100N), which is 100 times slower than O(N), would also be referred to as O(N).
The Role of Big O
Despite the fact that Big O doesn’t distinguish between Bubble Sort and Selection Sort, it is still very important, because it serves as a great way to classify the long-term growth rate of algorithms.
The Role of Big O
It is for this very reason that Big O ignores constants. The purpose of Big O is that for different classifications, there will be a point at which one classification supersedes the other in speed, and will remain faster forever. When that point occurs exactly, however, is not the concern of Big O.
The Role of Big O
So Big O is an extremely useful tool, because if two algorithms fall under different classifications of Big O, you’ll generally know which algorithm to use since with large amounts of data, one algorithm is guaranteed to be faster than the other at a certain point.
The Role of Big O
when two algorithms fall under the same classification of Big O, it doesn’t necessarily mean that both algorithms process at the same speed.
The Role of Big O
So while Big O is perfect for contrasting algorithms that fall under different classifications of Big O, when two algorithms fall under the same classification, further analysis is required to determine which algorithm is faster.
A Practical Example
A Practical Example
6. Optimizing for Optimistic Scenarios
However, we’ll discover in this chapter that the worst-case scenario isn’t the only situation worth considering. Being able to consider all scenarios is an important skill that can help you choose the appropriate algorithm for every situation.
The Efficiency of Insertion Sort
Big O Notation only takes into account the highest order of N.
The Average Case
Indeed, in a worst-case scenario, Selection Sort is faster than Insertion Sort. However, it is critical that we also take into account the average-case scenario.
The Average Case
By definition, the cases that occur most frequently are average scenarios. The worst- and best-case scenarios happen only rarely
The Average Case
the performance of Insertion Sort varies greatly based on the scenario. In the worst-case scenario, Insertion Sort takes N2 steps. In an average scenario, it takes N2 / 2 steps. And in the best-case scenario, it takes about N steps.
The Average Case
So which is better: Selection Sort or Insertion Sort? The answer is: well, it depends. In an average case—where an array is randomly sorted—they perform similarly. If you have reason to assume that you’ll be dealing with data that is mostly sorted, Insertion Sort will be a better choice. If you have reason to assume that you’ll be dealing with data that is mostly sorted in reverse order, Selection Sort will be faster. If you have no idea what the data will be like, that’s essentially an average case, and both will be equal.
Wrapping Up
Remember, while it’s good to be prepared for the worst case, average cases are what happen most of the time.

Enter the Hash Table
Most programming languages include a data structure called a hash table, and it has an amazing superpower: fast reading.
Enter the Hash Table
A hash table is a list of paired values.
Enter the Hash Table
Looking up a value in a hash table has an efficiency of O(1) on average, as it takes just one step
Hashing with Hash Functions
This process of taking characters and converting them to numbers is known as hashing. And the code that is used to convert those letters into particular numbers is called a hash function.
Building a Thesaurus for Fun and Profit, but Mainly Profit
Since our key ("bad") hashes into 8, the computer places the value ("evil") into cell 8:
Building a Thesaurus for Fun and Profit, but Mainly Profit
The computer then executes two simple steps: The computer hashes the key we’re looking up: BAD = 2 * 1 * 4 = 8. Since the result is 8, the computer looks inside cell 8 and returns the value that is stored there. In this case, that would be the string "evil".
Dealing with Collisions
Trying to add data to a cell that is already filled is known as a collision. Fortunately, there are ways around it.
Dealing with Collisions
It hashes the key. DAB = 4 * 1 * 2 = 8. It looks up cell 8. The computer takes note that cell 8 contains an array of arrays rather than a single value. It searches through the array linearly, looking at index 0 of each subarray until it finds the word we’re looking up ("dab"). It then returns the value at index 1 of the correct subarray.
Dealing with Collisions
In a scenario where the computer hits upon a cell that references an array, its search can take some extra steps, as it needs to conduct a linear search within an array of multiple values. If somehow all of our data ended up within a single cell of our hash table, our hash table would be no better than an array. So it actually turns out that the worst-case performance for a hash table lookup is O(N).
Dealing with Collisions
Because of this, it is critical that a hash table be designed in a way that it will have few collisions, and therefore typically perform lookups in O(1) time rather than O(N) time.
The Great Balancing Act
A good hash function, therefore, is one that distributes its data across all available cells
The Great Balancing Act
This is the balancing act that a hash table must perform. A good hash table strikes a balance of avoiding collisions while not consuming lots of memory.
The Great Balancing Act
To accomplish this, computer scientists have developed the following rule of thumb: for every seven data elements stored in a hash table, it should have ten cells.
The Great Balancing Act
This ratio of data to cells is called the load factor. Using this terminology, we’d say that the ideal load factor is 0.7 (7 elements / 10 cells)
Practical Examples
The truth is that hash tables are perfect for any situation where we want to keep track of which values exist within a dataset.
Wrapping Up
Hash tables are indispensable when it comes to building efficient software. With their O(1) reads and insertions, it’s a difficult data structure to beat.
8. Crafting Elegant Code with Stacks and Queues
stacks and queues are elegant tools for handling temporary data.
8. Crafting Elegant Code with Stacks and Queues
Stacks and queues allow you to handle data in order, and then get rid of it once you don’t need it anymore.
Stacks
A stack stores data in the same way that arrays do—it’s simply a list of elements. The one catch is that stacks have the following three constraints: Data can only be inserted at the end of a stack. Data can only be read from the end of a stack. Data can only be removed from the end of a stack.
Stacks
Inserting a new value into a stack is also called pushing onto the stack. Think of it as adding a dish onto the top of the dish stack.
Stacks
Note that we’re always adding data to the top (that is, the end) of the stack
Stacks
Removing elements from the top of the stack is called popping from the stack. Because of a stack’s restrictions, we can only pop data from the top.
Stacks
A handy acronym used to describe stack operations is LIFO, which stands for “Last In, First Out.”
Stacks
All this means is that the last item pushed onto a stack is always the first item popped from it.
Stacks in Action
Stacks are ideal for processing any data that should be handled in reverse order to how it was received (LIFO).
Queues
With queues, the first item added to the queue is the first item to be removed. That’s why computer scientists use the acronym “FIFO” when it comes to queues: First In, First Out.
Queues
Like stacks, queues are arrays with three restrictions (it’s just a different set of restrictions): Data can only be inserted at the end of a queue. (This is identical behavior as the stack.) Data can only be read from the front of a queue. (This is the opposite of behavior of the stack.) Data can only be removed from the front of a queue. (This, too, is the opposite behavior of the stack.)
9. Recursively Recurse with Recursion
Recursion is the official name for when a function calls itself.
Recurse Instead of Loop
In almost any case in which you can use a loop, you can also use recursion.
The Base Case
In Recursion Land (a real place), this case in which the method will not recurse is known as the base case. So in our countdown() function, 0 is the base case.
Reading Recursive Code
starting the analysis from the base case and building up is a great way to reason about recursive code.
Recursion in the Eyes of the Computer
The computer uses a stack to keep track of which functions it’s in the middle of calling. This stack is known, appropriately enough, as the call stack.
Recursion in the Eyes of the Computer
The computer begins by calling factorial(3). Before the method completes executing, however, factorial(2) gets called. In order to track that the computer is still in the middle of factorial(3), the computer pushes that info onto a call stack:
Recursion in the Eyes of the Computer
The computer then proceeds to execute factorial(2). Now, factorial(2), in turn, calls factorial(1). Before the computer dives into factorial(1), though, the computer needs to remember that it’s still in the middle of factorial(2), so it pushes that onto the call stack as well:
Recursion in the Eyes of the Computer
The computer then executes factorial(1). Since 1 is the base case, factorial(1) completes without the factorial method again.
Recursion in the Eyes of the Computer
Even after the computer completes factorial(1), it knows that it’s not finished with everything it needs to do since there’s data in the call stack, which indicates that it is still in the middle of running other methods that it needs to complete
Recursion in the Eyes of the Computer
factorial(3) is called first. factorial(2) is called second. factorial(1) is called third. factorial(1) is completed first. factorial(2) is completed based on the result of factorial(1). Finally, factorial(3) is completed based on the result of factorial(2).
Recursion in the Eyes of the Computer
Interestingly, in the case of infinite recursion (such as the very first example in our chapter), the program keeps on pushing the same method over and over again onto the call stack, until there’s no more room in the computer’s memory—and this causes an error known as stack overflow.
Recursion in Action
And this is the beauty of recursion. With recursion, we can write a script that goes arbitrarily deep—and is also simple!
10. Recursive Algorithms for Speed
And in many of these languages, the sorting algorithm that is employed under the hood is Quicksort.
10. Recursive Algorithms for Speed
Quicksort relies on a concept called partitioning, so we’ll jump into that first.
Partitioning
To partition an array is to take a random value from the array—which is then called the pivot—and make sure that every number that is less than the pivot ends up to the left of the pivot, and that every number that is greater than the pivot ends up to the right of the pivot.
Partitioning
we’ll always select the right-most value to be our pivot (although we can technically choose other values).
Partitioning
We then assign “pointers”—one to the left-most value of the array, and one to the right-most value of the array, excluding the pivot itself
Partitioning
We’re now ready to begin the actual partition, which follows these steps: The left pointer continuously moves one cell to the right until it reaches a value that is greater than or equal to the pivot, and then stops. Then, the right pointer continuously moves one cell to the left until it reaches a value that is less than or equal to the pivot, and then stops. We swap the values that the left and right pointers are pointing to. We continue this process until the pointers are pointing to the very same value or the left pointer has moved to the right of the right pointer. Finally, we swap the pivot with the value that the left pointer is currently pointing to.
Partitioning
When we’re done with a partition, we are now assured that all values to the left of the pivot are less than the pivot, and all values to the right of the pivot are greater than it. And that means that the pivot itself is now in its correct place within the array, although the other values are not yet necessarily completely sorted
Quicksort
The Quicksort algorithm relies heavily on partitions. It works as follows: Partition the array. The pivot is now in its proper place. Treat the subarrays to the left and right of the pivot as their own arrays, and recursively repeat steps #1 and #2. That means that we’ll partition each subarray, and end up with even smaller subarrays to the left and right of each subarray’s pivot. We then partition those subarrays, and so on and so forth. When we have a subarray that has zero or one elements, that is our base case and we do nothing.
Quicksort
The next step after the partition is to treat everything to the left of the pivot as its own array and partition it.
The Efficiency of Quicksort
Now, for randomly sorted data, there would be roughly half of N / 2 swaps, or N / 4 swaps. So with N comparisons and N / 4, we’d say there are about 1.25N steps. In Big O Notation, we ignore constants, so we’d say that a partition runs in O(N) time.
The Efficiency of Quicksort
In essence, we keep on breaking down each subarray into halves until we reach subarrays with elements of 1. How many times can we break up an array in this way? For an array of N, we can break it down log N times. That is, an array of 8 can be broken in half three times until we reach elements of 1. You’re already familiar with this concept from binary search.
Worst-Case Scenario
Since Big O ignores constants, we’d say that in a worst-case scenario, Quicksort has an efficiency of O(N2).
Worst-Case Scenario
We can see that they have identical worst-case scenarios, and that Insertion Sort is actually faster than Quicksort for a best-case scenario. However, the reason why Quicksort is so much more superior than Insertion Sort is because of the average scenario—which, again, is what happens most of the time. For average cases, Insertion Sort takes a whopping O(N2), while Quicksort is much faster at O(N log N).
Quickselect
Quickselect relies on partitioning just like Quicksort, and can be thought of as a hybrid of Quicksort and binary search.
Quickselect
This pivot is now in its correct spot, and since it’s in the fifth cell, we now know which value is the fifth-lowest value within the array. Now, we’re looking for the second-lowest value. But we also now know that the second-lowest value is somewhere to the left of the pivot. We can now ignore everything to the right of the pivot, and focus on the left subarray. It is in this respect that Quickselect is similar to binary search: we keep dividing the array in half, and focus on the half in which we know the value we’re seeking for will be found.
Quickselect
One of the beautiful things about Quickselect is that we can find the correct value without having to sort the entire array.
Quickselect
If we were to formulate this, we would say that for N elements, we would have N + (N/2) + (N/4) + (N/8) + … 2 steps. This always turns out to be roughly 2N steps. Since Big O ignores constants, we would say that Quickselect has an efficiency of O(N).
Linked Lists
When creating an array, your code finds a contiguous group of empty cells in memory and designates them to store data for your application
Linked Lists
Linked lists, on the other hand, do not consist of a bunch of memory cells in a row. Instead, they consist of a bunch of memory cells that are not next to each other, but can be spread across many different cells across the computer’s memory.
Linked Lists
This is the key to the linked list: in addition to the data stored within the node, each node also stores the memory address of the next node in the linked list.
Linked Lists
This extra piece of data—this pointer to the next node’s memory address—is known as a link.
Linked Lists
For our code to work with a linked list, all it really needs to know is where in memory the first node begins
Linked Lists
Since each node contains a link to the next node, if the application is given the first node of the linked list, it can piece together the rest of the list by following the link of the first node to the second node, and the link from the second node to the third node, and so on.
Linked Lists
One advantage of a linked list over an array is that the program doesn’t need to find a bunch of empty memory cells in a row to store its data.
Reading
Instead, all the program knows is the memory address of the first node of the linked list.
Reading
To find index 2 (which is the third node), the program must begin looking up index 0 of the linked list, and then follow the link at index 0 to index 1. It must then again follow the link at index 1 to index 2, and finally inspect the value at index 2.
Reading
If we ask a computer to look up the value at a particular index, the worst-case scenario would be if we’re looking up the last index in our list. In such a case, the computer will take N steps to look up this index, since it needs to start at the first node of the list and follow each link until it reaches the final node. Since Big O Notation expresses the worst-case scenario unless explicitly stated otherwise, we would say that reading a linked list has an efficiency of O(N). This is a significant disadvantage in comparison with arrays in which reads are O(1).
Searching
Arrays and linked lists have the same efficiency for search. Remember, searching is looking for a particular piece of data within the list and getting its index. With both arrays and linked lists, the program needs to start at the first cell and look through each and every cell until it finds the value it’s searching for. In a worst-case scenario—where the value we’re searching for is in the final cell or not in the list altogether—it would take O(N) steps.
Insertion
nsertion is one operation in which linked lists can have a distinct advantage over arrays in certain situations. Recall that the worst-case scenario for insertion into an array is when the program inserts data into index 0, because it then has to shift the rest of the data one cell to the right, which ends up yielding an efficiency of O(N). With linked lists, however, insertion at the beginning of the list takes just one step—which is O(1)
Insertion
Therefore, inserting into the middle of a linked list takes O(N), just as it does for an array.
Insertion
That is, inserting at the beginning is great for linked lists, but terrible for arrays. And inserting at the end is an array’s best-case scenario, but the worst case when it comes to a linked list.
Insertion
Scenario Array Linked list Insert at beginning Worst case Best case Insert at middle Average case Average case Insert at end Best case Worst case
Deletion
Deletion is very similar to insertion in terms of efficiency. To delete a node from the beginning of a linked list, all we need to do is perform one step: we change the first_node of the linked list to now point to the second node.
Deletion
When it comes to deleting the final node of a linked list, the actual deletion takes one step—we just take the second-to-last node and make its link null. However, it takes N steps to first get to the second-to-last node, since we need to start at the beginning of the list and follow the links until we reach it.
Deletion
Situation Array Linked list Delete at beginning Worst case Best case Delete at middle Average case Average case Delete at end Best case Worst case
Deletion
Operation Array Linked list Reading O(1) O(N) Search O(N) O(N) Insertion O(N) (O(1) at end) O(N) (O(1) at beginning) Deletion O(N) (O(1) at end) O(N) (O(1) at beginning)
Linked Lists in Action
With a linked list, however, as we comb through the list, each deletion takes just one step, as we can simply change a node’s link to point to the appropriate node and move on.
Doubly Linked Lists
However, if we use a special variant of a linked list called the doubly linked list, we’d be able to insert and delete data from a queue at O(1).
Doubly Linked Lists
A doubly linked list is like a linked list, except that each node has two links—one that points to the next node, and one that points to the preceding node. In addition, the doubly linked list keeps track of both the first and last nodes.
Doubly Linked Lists
Since a doubly linked list always knows where both its first and last nodes are, we can access each of them in a single step, or O(1).
Doubly Linked Lists
Because doubly linked lists have immediate access to both the front and end of the list, they can insert data on either side at O(1) as well as delete data on either side at O(1). Since doubly linked lists can insert data at the end in O(1) time and delete data from the front in O(1) time, they make the perfect underlying data structure for a queue.
Binary Trees
A tree is also a node-based data structure, but within a tree, each node can have links to multiple nodes.
Binary Trees
The uppermost node (in our example, the “j”) is called the root
Binary Trees
A binary tree is a tree that abides by the following rules: Each node has either zero, one, or two children. If a node has two children, it must have one child that has a lesser value than the parent, and one child that has a greater value than the parent.
Searching
Inspect the value at the node. If we’ve found the value we’re looking for, great! If the value we’re looking for is less than the current node, search for it in its left subtree. If the value we’re looking for is greater than the current node, search for it in its right subtree.
Searching
More generally, we’d say that searching in a binary tree is O(log N). This is because each step we take eliminates half of the remaining possible values in which our value can be stored. (We’ll see soon, though, that this is only for a perfectly balanced binary tree, which is a best-case scenario.)
Insertion
Insertion always takes just one extra step beyond a search, which means that insertion takes log N + 1 steps, which is O(log N) as Big O ignores constants.
Insertion
Because of this, if you ever wanted to convert an ordered array into a binary tree, you’d better first randomize the order of the data.
Insertion
It emerges that in a worst-case scenario, where a tree is completely imbalanced, search is O(N). In a best-case scenario, where it is perfectly balanced, search is O(log N). In the typical scenario, in which data is inserted in random order, a tree will be pretty well balanced and search will take about O(log N).
Deletion
Pulling all the steps together, the algorithm for deletion from a binary tree is: If the node being deleted has no children, simply delete it. If the node being deleted has one child, delete it and plug the child into the spot where the deleted node was. When deleting a node with two children, replace the deleted node with the successor node. The successor node is the child node whose value is the least of all values that are greater than the deleted node. If the successor node has a right child, after plugging the successor node into the spot of the deleted node, take the right child of the successor node and turn it into the left child of the parent of the successor node.
Deletion
Like search and insertion, deleting from trees is also typically O(log N)
Binary Trees in Action
The process of visiting every node in a data structure is known as traversing the data structure.
Graphs
A graph is a data structure that specializes in relationships, as it easily conveys how data is connected.
Graphs
Each person is represented by a node, and each line indicates a friendship with another person. In graph jargon, each node is called a vertex, and each line is called an edge. Vertices that are connected by an edge are said to be adjacent to each other.
Graphs
Because relationships in Twitter are one-directional, we use arrows in our visual implementation, and such a graph is known as a directed graph. In Facebook, where the relationships are mutual and we use simple lines, the graph is called a non-directed graph.
Breadth-First Search
We remove a vertex from the queue to designate it as the current vertex. For each current vertex, we visit each of its adjacent vertices.
Breadth-First Search
Now, each vertex ends up being removed from the queue once. In Big O Notation, this is called O(V). That is, for V vertices in the graph, there are V removals from the queue.
Breadth-First Search
Since there are O(V) removals from the queue, and O(E) visits, we say that breadth-first search has an efficiency of O(V + E).
Weighted Graphs
A weighted graph is like a regular graph but contains additional information about the edges in the graph.
Dijkstra’s Algorithm
Here are the rules of Dijkstra’s algorithm (don’t worry—they’ll become clearer when we walk through our example): We make the starting vertex our current vertex. We check all the vertices adjacent to the current vertex and calculate and record the weights from the starting vertex to all known locations. To determine the next current vertex, we find the cheapest unvisited known vertex that can be reached from our starting vertex. Repeat the first three steps until we have visited every vertex in the graph.
14. Dealing with Space Constraints
There are situations, however, where we need to measure algorithm efficiency by another measure known as space complexity, which is how much memory an algorithm consumes.
Big O Notation as Applied to Space Complexity
Interestingly, computer scientists use Big O Notation to describe space complexity just as they do to describe time complexity.
Big O Notation as Applied to Space Complexity
Since this function does not consume any memory in addition to the original array, we’d describe the space complexity of this function as being O(1).
Big O Notation as Applied to Space Complexity
In our case, the algorithm consumes the same amount of additional space (zero!) no matter whether the original array contains four elements or one hundred. Because of this, our new version of makeUpperCase() is said to have a space efficiency of O(1).
Big O Notation as Applied to Space Complexity
It’s important to reiterate that in this book, we judge the space complexity based on additional memory consumed—known as auxiliary space—meaning that we don’t count the original data
