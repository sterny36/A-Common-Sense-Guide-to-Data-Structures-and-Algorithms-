{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Common-Sense Guide to Data Structures and Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Why Data Structures Matter\n",
    "Most data structures are used in four basic ways, which we refer to as operations:\n",
    "\n",
    "1. Read\n",
    "2. Search\n",
    "3. Insert\n",
    "4. Delete\n",
    "----\n",
    "- When we measure how “fast” an operation takes, we do not refer to how fast the operation takes in terms of pure time, but instead in how many steps it takes.\n",
    "- Measuring the speed of an operation is also known as measuring its time complexity. Throughout this book, we’ll use the terms speed, time complexity, efficiency, and performance interchangeably. They all refer to the number of steps that a given operation takes.\n",
    "\n",
    "### Arrays\n",
    "- When a program declares an array, it allocates a contiguous set of empty cells for use in the program.\n",
    "- Every cell in a computer’s memory has a specific address.\n",
    "- Each cell’s memory address is one number greater than the previous cell.\n",
    "#### Reading \n",
    "- When a computer reads the value at a particular index it can jumps straight to that index. \n",
    "- Reading from an array is, therefore, a very efficient operation, since it takes just one step. An operation with just one step is naturally the fastest type of operation.\n",
    "#### Searching \n",
    "- Searching an array is looking to see whether a particular value exists within an array and if so, which index it’s located at.\n",
    "- To search for a value within an array, the computer starts at index 0, checks the value, and if it doesn’t find what it’s looking for, moves on to the next index. It does this until it finds the value it’s seeking.\n",
    "- Another way of saying this is that for N cells in an array, linear search will take a maximum of N steps. In this context, N is just a variable that can be replaced by any number.\n",
    "#### Insertion\n",
    "- “The efficiency of inserting a new piece of data inside an array depends on where inside the array you’d like to insert it.\n",
    "- Appending to the end of an array just takes one step.\n",
    "- Inserting a piece of data into the array takes more steps as first all subsequent data needs to be shifted by one to make room for the new item.\n",
    "- The worst-case scenario for insertion into an array—that is, the scenario in which insertion takes the most steps—is where we insert data at the beginning of the array. This is because when inserting into the beginning of the array, we have to move all the other values one cell to the right.\n",
    "- So we can say that insertion in a worst-case scenario can take up to N + 1 steps for an array containing N elements. This is because the worst-case scenario is inserting a value into the beginning of the array in which there are N shifts (every data element of the array) and one insertion.\n",
    "#### Deletion\n",
    "- Arrays can't have empty values. Therefore when removing an item, all subsequent items need to be shifted to fill the gap. \n",
    "- Like insertion, the worst-case scenario of deleting an element is deleting the very first element of the array.\n",
    "- We can conclude, then, that for an array containing N elements, the maximum number of steps that deletion would take is N steps.\n",
    "\n",
    "### Sets\n",
    "- A set is an array with one simple constraint of not allowing duplicates. Yet, this constraint actually causes the set to have a different efficiency for one of the four primary operations.\n",
    "#### Reading\n",
    "- Reading from a set is exactly the same as reading from an array—it takes just one step for the computer to look up what is contained within a particular index.\n",
    "\n",
    "#### Searching \n",
    "- Searching a set also turns out to be no different than searching an array. It takes up to N steps to search to see if a value exists within a set. \n",
    "\n",
    "#### Deletion \n",
    "- Deletion is also identical between a set and an array—it takes up to N steps to delete a value and move data to the left to close the gap.\n",
    "\n",
    "#### Insertion \n",
    "- Insertion is where arrays and sets diverge. \n",
    "- Inserting a value at the end of a set:  With an array, the computer can insert a value at its end in a single step. With a set, however, the computer first needs to determine that this value doesn’t already exist in this set—because that’s what sets do: they prevent duplicate data. So every insert first requires a search.\n",
    "- Insertion into a set in a best-case scenario will take N + 1 steps for N elements. This is because there are N steps of search to ensure that the value doesn’t already exist within the set, and then one step for the actual insertion.”\n",
    "- In a worst-case scenario, where we’re inserting a value at the beginning of a set, the computer needs to search N cells to ensure that the set doesn’t already contain that value, and then another N steps to shift all the data to the right, and another final step to insert the new value. That’s a total of 2N + 1 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Why Algorithms matter\n",
    "-“ An algorithm is simply a particular process for solving a problem. ”\n",
    "### Ordered Arrays\n",
    "- The ordered array is almost identical to the array we discussed in the previous chapter. \n",
    "- The only difference is that ordered arrays require that the values are always kept—you guessed it—in order. \n",
    "- That is, every time a value is added, it gets placed in the proper cell so that the values of the array remain sorted.\n",
    "#### Insertion\n",
    "- When inserting into an ordered array, we need to always conduct a search before the actual insertion to determine the correct spot for the insertion. That is one key difference (in terms of efficiency) between a standard array and an ordered array.\n",
    "#### Searching\n",
    "- The major advantage of an ordered array over a standard array is that we have the option of performing a binary search rather than a linear search. Binary search is impossible with a standard array because the values can be in any order.\n",
    "\n",
    "```ruby\n",
    "def binary_search(array, value)\n",
    " \t\n",
    "    # First, we establish the lower and upper bounds of where the value\n",
    " \t# we're searching for can be. To start, the lower bound is the first\n",
    " \t# value in the array, while the upper bound is the last value:\n",
    " \tlower_bound = 0\n",
    " \tupper_bound = array.length - 1\n",
    " \t\n",
    " \t# We begin a loop in which we keep inspecting the middlemost value\n",
    " \t# between the upper and lower bounds:\n",
    " \twhile lower_bound <= upper_bound do\n",
    " \t\n",
    "        # We find the midpoint between the upper and lower bounds:\n",
    "        # (We don't have to worry about the result being a non-integer\n",
    "        # since in Ruby, the result of division of integers will always\n",
    "        # be rounded down to the nearest integer.)\n",
    "        midpoint = (upper_bound + lower_bound) / 2\n",
    " \t\n",
    " \t    # We inspect the value at the midpoint:\n",
    " \t    value_at_midpoint = array[midpoint]\n",
    " \t\n",
    " \t    # If the value at the midpoint is the one we're looking for, we're done.\n",
    " \t    # If not, we change the lower or upper bound based on whether we need\n",
    " \t    # to guess higher or lower:\n",
    " \t    if value < value_at_midpoint\n",
    " \t      upper_bound = midpoint - 1\n",
    " \t    elsif value > value_at_midpoint\n",
    " \t      lower_bound = midpoint + 1\n",
    " \t    elsif value == value_at_midpoint\n",
    " \t      return midpoint\n",
    " \t    end\n",
    " \t  end\n",
    " \t\n",
    "  # If we've narrowed the bounds until they've reached each other, that\n",
    "  # means that the value we're searching for is not contained within\n",
    "  # this array:\n",
    "  return nil\n",
    "end\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Big O Notation\n",
    "- Big O Notation formalizes expression around these concepts and allows us to easily categorize the efficiency of a given algorithm and convey it to others.\n",
    "- Instead of focusing on units of time, Big O achieves consistency by focusing only on the number of steps that an algorithm takes.\n",
    "- Reading from an array takes just one step, no matter how large the array is. The way to express this in Big O Notation is: O(1)\n",
    "    - O(1) simply means that the algorithm takes the same number of steps no matter how much data there is. In this case, reading from an array always takes just one step no matter how much data the array contains.\n",
    "- Linear search: For N elements in the array, linear search can take up to a maximum of N steps, which means it is: O(N)\n",
    "\n",
    "### Constant Time vs. Linear Time\n",
    "- Big O Notation does more than simply describe the number of steps that an algorithm takes, such as a hard number such as 22 or 400. Rather, it describes how many steps an algorithm takes based on the number of data elements that the algorithm is acting upon. Another way of saying this is that Big O answers the following question: how does the number of steps change as the data increases?\n",
    "- As Big O is primarily concerned about how an algorithm performs across varying amounts of data, an important point emerges: an algorithm can be described as O(1) even if it takes more than one step. Let’s say that a particular algorithm always takes three steps, rather than one—but it always takes these three steps no matter how much data there is.\n",
    "- Because the number of steps remains constant no matter how much data there is, this would also be considered constant time and be described by Big O Notation as O(1). Even though the algorithm technically takes three steps rather than one step, Big O Notation considers that trivial. O(1) is the way to describe any algorithm that doesn’t change its number of steps even when the data increases.\n",
    "- While a 100-step algorithm is less efficient than a one-step algorithm, the fact that it is O(1) still makes it more efficient than any O(N) algorithm. Why is this?\n",
    "- Because there will always be some amount of data for which O(N) takes more steps from that point until infinity, O(N) is considered to be, on the whole, less efficient than O(1).\n",
    "- The same is true for an O(1) algorithm that always takes one million steps. As the data increases, there will inevitably reach a point where O(N) becomes less efficient than the O(1) algorithm, and will remain so up until an infinite amount of data.\n",
    "- While Big O effectively describes both the best- and worst-case scenarios of a given algorithm, Big O Notation generally refers to worst-case scenario unless specified otherwise. This is why most references will describe linear search as being O(N) even though it can be O(1) in a best-case scenario.\n",
    "\n",
    "### Binary search \n",
    "- Binary search seems to fall somewhere in between O(1) and O(N).\n",
    "- In Big O, we describe binary search as having a time complexity of: O(log N).\n",
    "- Simply put, O(log N) is the Big O way of describing an algorithm that increases one step each time the data is doubled.\n",
    "\n",
    "### Logarithms\n",
    "- Logarithms are the inverse of exponents.\n",
    "- 2^3 is the equivalent of: 2 * 2 * 2 which just happens to be 8. \n",
    "- log2 8 is the converse of the above. It means: how many times do you have to multiply 2 by itself to get a result of 8? Since you have to multiply 2 by itself 3 times to get 8, log2 8 = 3.\n",
    "- Another way of explaining log2 8 is: if we kept dividing 8 by 2 until we ended up with 1, how often would we need to divide? 8 / 2 / 2 / 2 = 1 In this example, it takes us 3 times.\n",
    "- O(log N) means that the algorithm takes as many steps as it takes to keep halving the data elements until we remain with one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 & 5: Sorting and Big O\n",
    "Sorting algorithms have been the subject of extensive research in computer science, and tens of such algorithms have been developed over the years. They all solve the following problem:\n",
    "\n",
    "> Given an array of unsorted numbers, how can we sort them so that they end up in ascending order?\n",
    "\n",
    "### Bubble sort\n",
    "Bubble Sort is a very basic sorting algorithm, and follows these steps:\n",
    "\n",
    "1. Point to two consecutive items in the array. (Initially, we start at the very beginning of the array and point to its first two items.) Compare the first item with the second one. \n",
    "2. If the two items are out of order (in other words, the left value is greater than the right value), swap them. (If they already happen to be in the correct order, do nothing for this step.)\n",
    "3. Move the “pointers” one cell to the right. Repeat steps 1 and 2 until we reach the end of the array or any items that have already been sorted.\n",
    "4. Repeat steps 1 through 3 until we have a round in which we didn’t have to make any swaps. This means that the array is in order.\n",
    "\n",
    "Each time we repeat steps 1 through 3 is known as a passthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = [4,2,8,3,1,5,7,6,9]\n",
    "\n",
    "passthrough_count = 0\n",
    "\n",
    "until passthrough_count == array.size\n",
    "  array.each_with_index do |item, idx|\n",
    "    break if idx + 1 == array.size - passthrough_count\n",
    "    if array[idx] > array[idx+1]\n",
    "      array[idx], array[idx+1] = array[idx+1], array[idx]\n",
    "    end\n",
    "  end\n",
    "  passthrough_count += 1\n",
    "end\n",
    "p array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Efficiency of Bubble Sort\n",
    "The Bubble Sort algorithm contains two kinds of steps:\n",
    "- Comparisons: two numbers are compared with one another to determine which is greater.\n",
    "- Swaps: two numbers are swapped with one another in order to sort them.\n",
    "\n",
    "\n",
    "With an array containing ten elements in reverse order, we’d have: 9 + 8 + 7 + 6 + 5 + 4 + 3 + 2 + 1 = 45 comparisons, and another forty-five swaps. That’s a total of ninety steps.\n",
    "As the number of elements increase, the number of steps grows exponentially.\n",
    "If you look precisely at the growth of steps as N increases, you’ll see that it’s growing by approximately N^2.\n",
    "Therefore, in Big O Notation, we would say that Bubble Sort has an efficiency of O(N^2).\n",
    "Said more officially: in an O(N^2) algorithm, for N data elements, there are roughly N^2 steps.\n",
    "O(N^2) is also referred to as quadratic time.\n",
    "\n",
    "- Unsurprisingly, O(N2) is the efficiency of algorithms in which nested loops are used. \n",
    "- When you see a nested loop, O(N^2) alarm bells should start going off in your head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Selection sort\n",
    "1. We check each cell of the array from left to right to determine which value is least. As we move from cell to cell, we keep in a variable the lowest value we’ve encountered so far. (Really, we keep track of its index, but for the purposes of the following diagrams, we’ll just focus on the actual value.) If we encounter a cell that contains a value that is even less than the one in our variable, we replace it so that the variable now points to the new index. \n",
    "2. Once we’ve determined which index contains the lowest value, we swap that index with the value we began the passthrough with. This would be index 0 in the first passthrough, index 1 in the second passthrough, and so on and so forth. In the next diagram, we make the swap of the first passthrough:\n",
    "3. Repeat steps 1 and 2 until all the data is sorted.\n",
    "\n",
    "### The efficiency of selection sort\n",
    "- Selection Sort contains two types of steps: comparisons and swaps.\n",
    "- Comparisons: For N elements, we make (N - 1) + (N - 2) + (N - 3) … + 1 comparisons.\n",
    "- Swaps: We only need to make a maximum of one swap per passthrough.\n",
    "- --> Selection Sort contains about half the number of steps that Bubble Sort does, indicating that Selection Sort is twice as fast.\n",
    "\n",
    "### Selection sort implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = [4,2,8,3,1,5,7,6,9]\n",
    "\n",
    "array.each_with_index do |outer, odx|\n",
    "  index_min = odx\n",
    "\n",
    "  # set index_min to the index that points to the smallest value\n",
    "  for idx in (odx + 1...array.size) do \n",
    "      index_min = idx if array[idx] < array[index_min]\n",
    "  end\n",
    "  \n",
    "  # if value needs to be swapped, swap \n",
    "  array[index_min], array[odx] = array[odx], array[index_min] if index_min != odx\n",
    "  \n",
    "end\n",
    "\n",
    "\n",
    "# or\n",
    "n = array.size - 1\n",
    "\n",
    "n.times do |i|\n",
    "  index_min = i\n",
    "\n",
    "  (i + 1).upto(n) do |j|\n",
    "    index_min = j if array[j] < array[index_min]\n",
    "  end\n",
    "  \n",
    "  array[i], array[index_min] = array[index_min], array[i] if index_min != i\n",
    "end\n",
    "\n",
    "p array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big O ignores constants\n",
    "- However, in the world of Big O Notation, Selection Sort and Bubble Sort are described in exactly the same way.\n",
    "- In reality, Selection Sort is described in Big O as O(N^2), just like Bubble Sort. \n",
    "- Rule: Big O Notation ignores constants.\n",
    "- i.e. O(N2 / 2) becomes simply O(N2). Similarly, O(2N) would become O(N), and O(N / 2) would also become O(N). \n",
    "- Despite the fact that Big O doesn’t distinguish between Bubble Sort and Selection Sort, it is still very important, because it serves as a great way to classify the long-term growth rate of algorithms.\n",
    "- It is for this very reason that Big O ignores constants. \n",
    "- The purpose of Big O is that for different classifications, there will be a point at which one classification supersedes the other in speed, and will remain faster forever. \n",
    "- When that point occurs exactly, however, is not the concern of Big O.\n",
    "- So while Big O is perfect for contrasting algorithms that fall under different classifications of Big O, when two algorithms fall under the same classification, further analysis is required to determine which algorithm is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Optimistic scenarios\n",
    "- Big O notations usually focuses on the worst case performace of an algorithm. \n",
    "- However, it is also important to analyse algorithms for their best case and specifically for their average case performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion Sort\n",
    "1. In the first passthrough, we temporarily remove the value at index 1 (the second cell) and store it in a temporary variable. This will leave a gap at that index, since it contains no value. In subsequent passthroughs, we remove the values at the subsequent indexes.\n",
    "2. We then begin a shifting phase, where we take each value to the left of the gap, and compare it to the value in the temporary variable: If the value to the left of the gap is greater than the temporary variable, we shift that value to the right: As we shift values to the right, inherently, the gap moves leftwards. As soon as we encounter a value that is lower than the temporarily removed value, or we reach the left end of the array, this shifting phase is over.\n",
    "3. We then insert the temporarily removed value into the current gap:\n",
    "4. We repeat steps 1 through 3 until the array is fully sorted.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def insertion_sort(array)\n",
    "  array.each_with_index do |item, idx|\n",
    "    position = idx\n",
    "    temp_value = item\n",
    "    \n",
    "    while position > 0 and array[position - 1] > temp_value do \n",
    "      array[position] = array[position - 1]\n",
    "      position = position - 1\n",
    "    end\n",
    "    \n",
    "    array[position] = temp_value\n",
    "    \n",
    "  end\n",
    "end\n",
    "\n",
    "array = [4,2,8,3,1,5,7,6,9]\n",
    "p insertion_sort(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a worst-case scenario, Selection Sort is faster than Insertion Sort.\n",
    "- However, it is critical that we also take into account the average-case scenario as by definition the cases that occur most frequently are average scenarios. The worst- and best-case scenarios happen only rarely.\n",
    "- The performance of Insertion Sort varies greatly based on the scenario. In the worst-case scenario, Insertion Sort takes N^2 steps. In an average scenario, it takes N^2 / 2 steps. And in the best-case scenario, it takes about N steps.\n",
    "- Selection Sort takes N2 / 2 steps in all cases, from worst to average to best-case scenarios. This is because Selection Sort doesn’t have any mechanism for ending a passthrough early at any point.\n",
    "- So which is better, Selection Sort or Insertion Sort? The answer is: It depends. \n",
    "  - In an average case—where an array is randomly sorted—they perform similarly. \n",
    "  - If you have reason to assume that you’ll be dealing with data that is mostly sorted, Insertion Sort will be a better choice. \n",
    "  - If you have reason to assume that you’ll be dealing with data that is mostly sorted in reverse order, Selection Sort will be faster. \n",
    "  - If you have no idea what the data will be like, that’s essentially an average case, and both will be equal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: Hash Tables\n",
    "- Most programming languages include a data structure called a hash table, and it has an amazing superpower: fast reading.\n",
    "- A hash table is a list of paired values.\n",
    "- Looking up a value in a hash table has an efficiency of O(1) on average, as it takes just one step.\n",
    "- Hash tables use hash function to determine which memory field to place a value in.\n",
    "    - For example: If the key \"bad\" hashes into 8, the value \"evil\" is placed into cell 8.\n",
    "- To lookup a value, the key is hashed again and the value from the resulting cell is returned.  \n",
    "\n",
    "### Dealing with collissions: \n",
    "- If multiple keys hash to the same cell index, the computer can store an array in that cell that contains subarrays containing the key/value pairs. \n",
    "- In a scenario where the computer hits upon a cell that references an array, its search can take some extra steps, as it needs to conduct a linear search within an array of multiple values. If somehow all of our data ended up within a single cell of our hash table, our hash table would be no better than an array. So it actually turns out that the worst-case performance for a hash table lookup is O(N).\n",
    "- Because of this, it is critical that a hash table be designed in a way that it will have few collisions, and therefore typically perform lookups in O(1) time rather than O(N) time.\n",
    "- A good hash function, therefore, is one that distributes its data across all available cells\n",
    "- This is the balancing act that a hash table must perform. A good hash table strikes a balance of avoiding collisions while not consuming lots of memory.\n",
    "- To accomplish this, computer scientists have developed the following rule of thumb: for every seven data elements stored in a hash table, it should have ten cells.\n",
    "- This ratio of data to cells is called the load factor. Using this terminology, we’d say that the ideal load factor is 0.7 (7 elements / 10 cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8: Stacks & Queues\n",
    "- Stacks and queues are elegant tools for handling temporary data.\n",
    "- Stacks and queues allow you to handle data in order, and then get rid of it once you don’t need it anymore.\n",
    "### Stacks\n",
    "- A stack stores data in the same way that arrays do—it’s simply a list of elements. \n",
    "- However, stacks have the following three constraints: \n",
    "  - Data can only be inserted at the end of a stack. \n",
    "  - Data can only be read from the end of a stack. \n",
    "  - Data can only be removed from the end of a stack.\n",
    "- Inserting a new value into a stack is also called pushing onto the stack. Think of it as adding a dish onto the top of the dish stack.\n",
    "- Note that we’re always adding data to the top (that is, the end) of the stack\n",
    "- Removing elements from the top of the stack is called popping from the stack. Because of a stack’s restrictions, we can only pop data from the top.\n",
    "- A handy acronym used to describe stack operations is LIFO, which stands for “Last In, First Out.”\n",
    "- All this means is that the last item pushed onto a stack is always the first item popped from it.\n",
    "- Stacks are ideal for processing any data that should be handled in reverse order to how it was received (LIFO).\n",
    "\n",
    "### Queues\n",
    "- With queues, the first item added to the queue is the first item to be removed. \n",
    "- Queues are “FIFO”: First In, First Out.\n",
    "- Like stacks, queues are arrays with three restrictions (it’s just a different set of restrictions): \n",
    "  - Data can only be inserted at the end of a queue. (This is identical behavior as the stack.) \n",
    "  - Data can only be read from the front of a queue. (This is the opposite of behavior of the stack.) \n",
    "  - Data can only be removed from the front of a queue. (This, too, is the opposite behavior of the stack.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9: Recursion \n",
    "- Recursion is the official name for when a function calls itself.\n",
    "- In almost any case in which you can use a loop, you can also use recursion.\n",
    "- The Base Case is the case in which the method will not recurse is known as the base case. \n",
    "- It's often easiest to start the analysis from the base case and then go in reverse to reason about recursive code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A basic example of a recursive funciton\n",
    "def factorial(number)\n",
    "  if number == 1 # the base case\n",
    "    return 1\n",
    "  else \n",
    "    return number * factorial(number - 1)\n",
    "  end\n",
    "end\n",
    "\n",
    "factorial(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursion in the Eyes of the Computer\n",
    "- Example: `factorial(3)`\n",
    "- The computer uses a stack to keep track of which functions it’s in the middle of calling. This stack is known, appropriately enough, as the call stack.\n",
    "---\n",
    "1. The computer begins by calling factorial(3). \n",
    "2. Before the method completes executing, however, factorial(2) gets called. In order to track that the computer is still in the middle of factorial(3), the computer pushes that info onto a call stack.\n",
    "3. The computer then proceeds to execute factorial(2). Now, factorial(2), in turn, calls factorial(1). Before the computer dives into factorial(1), though, the computer needs to remember that it’s still in the middle of factorial(2), so it pushes that onto the call stack as well.\n",
    "4. The computer then executes factorial(1). Since 1 is the base case, factorial(1) completes without the factorial method again.\n",
    "5. Even after the computer completes factorial(1), it knows that it’s not finished with everything it needs to do since there’s data in the call stack, which indicates that it is still in the middle of running other methods that it needs to complete. \n",
    "6. Since factorial(2) is the last item in the call stack, that means that factorial(2) is the most recently called method and therefore the immediate method that needs to be completed. The computer then pops factorial(2) from the call stack and completes running factorial(2).\n",
    "7. Then the computer looks at the call stack to see which method it needs to complete next, and since the call stack is currently it pops factorial(3) from the stack and completes it.\n",
    "8. At this point, the stack is empty, so the computer knows it’s done executing all of the methods, and the recursion is complete.\n",
    "--- \n",
    "1. factorial(3) is called first. \n",
    "2. factorial(2) is called second. \n",
    "3. factorial(1) is called third. \n",
    "1. factorial(1) is completed first. \n",
    "2. factorial(2) is completed based on the result of factorial(1). \n",
    "3. Finally, factorial(3) is completed based on the result of factorial(2).\n",
    "---\n",
    "- Interestingly, in the case of infinite recursion the program keeps on pushing the same method over and over again onto the call stack, until there’s no more room in the computer’s memory—and this causes an error known as stack overflow.\n",
    "- The beauty of recursion is that we can write a script that goes arbitrarily deep—and is also simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 10: Partitioning, Quicksort, Quickselect\n",
    "\n",
    "- In many languages, the standard sorting algorithm that is employed under the hood is Quicksort.\n",
    "- Quicksort relies on a concept called partitioning.\n",
    "\n",
    "### Partitioning\n",
    "- To partition an array is to take a random value from the array—which is then called the pivot—and make sure that every number that is less than the pivot ends up to the left of the pivot, and that every number that is greater than the pivot ends up to the right of the pivot, i.e. to position the pivot in the correct position. Any value can be chosen as the pivot. \n",
    "\n",
    "---\n",
    "Preperation: \n",
    "1. Select a pivot, for example the right-most value in the array.\n",
    "2. Set 2x pointers: First on the leftmost item in the array, the other on the rightmost value (excl. pivot).\n",
    "---\n",
    "Partitioning:\n",
    "1. The left pointer continuously moves one cell to the right until it reaches a value that is greater than or equal to the pivot, and then stops. \n",
    "2. The right pointer continuously moves one cell to the left until it reaches a value that is less than or equal to the pivot, and then stops. \n",
    "3. Swap the values that the left and right pointers are pointing to.\n",
    "4. Continue this process until the pointers are pointing to the very same value or the left pointer has moved to the right of the right pointer. \n",
    "5. Finally, swap the pivot with the value that the left pointer is currently pointing to.\n",
    "---\n",
    "- When we’re done with a partition, we are now assured that all values to the left of the pivot are less than the pivot, and all values to the right of the pivot are greater than it. \n",
    "- That means that the pivot itself is now in its correct place within the array.\n",
    "- The other values are not yet necessarily completely sorted!\n",
    "\n",
    "### Quicksort\n",
    "- The Quicksort algorithm relies heavily on partitions. \n",
    "---\n",
    "1. Partition the array. The pivot is now in its proper place.\n",
    "2. Treat the subarrays to the left and right of the pivot as their own arrays, and recursively repeat steps #1 and #2. That means that we’ll partition each subarray, and end up with even smaller subarrays to the left and right of each subarray’s pivot. We then partition those subarrays, and so on and so forth. \n",
    "3. When we have a subarray that has zero or one elements, that is our base case and we do nothing.\n",
    "\n",
    "### The Efficiency of Quicksort\n",
    "- A partition consists of two types of steps:\n",
    "    - Comparisons: We compare each value to the pivot (N comparisons).\n",
    "    - Swaps: When appropriate, we swap the values being pointed to by the left and right pointers. (Worst case: N/2, Avg case: N/4) \n",
    "- So with N comparisons and N / 4, we’d say there are about 1.25N steps. In Big O Notation, we ignore constants, so we’d say that a partition runs in O(N) time.\n",
    "- So that’s the efficiency of a single partition. But Quicksort involves many partitions on arrays and subarrays of different sizes, so we need to do a further analysis to determine the efficiency of Quicksort.\n",
    "- If we do this analysis for arrays of various sizes, we’ll see that for N elements, there are about N * log N steps.\n",
    "- Since we can break up the original array into equal subarrays log N times, and for each time we break them up, we must run partitions on all N cells from the original array, we end up with about N * log N steps.\n",
    "- For Quicksort, the best-case scenario is one in which the pivot always ends up smack in the middle of the subarray after the partition.\n",
    "- Fr Quicksort the worst-case scenario is one in which the pivot always ends up on one side of the subarray instead of the middle. \n",
    "- For average cases, Insertion Sort takes a whopping O(N2), while Quicksort is much faster at O(N log N).\n",
    "\n",
    "### Quickselect\n",
    "- Quickselect relies on partitioning just like Quicksort, and can be thought of as a hybrid of Quicksort and binary search.\n",
    "- One of the beautiful things about Quickselect is that we can find the correct value without having to sort the entire array.\n",
    "- As we’ve seen earlier in this chapter, after a partition, the pivot value ends up in the appropriate spot in the array. \n",
    "\n",
    "Example: Array containing 8 elemcents, find the second-to-lowest element. \n",
    "1. Partition array, pivot will probably end up somewhere in the middle.\n",
    "2. This pivot is now in its correct spot, and since it’s in the fifth cell, we now know which value is the fifth-lowest value within the array. Now, we’re looking for the second-lowest value. But we also now know that the second-lowest value is somewhere to the left of the pivot.\n",
    "3. We can now ignore everything to the right of the pivot, and focus on the left subarray. It is in this respect that Quickselect is similar to binary search: we keep dividing the array in half, and focus on the half in which we know the value we’re seeking for will be found.\n",
    "4. Repeat steps 1 to 3 until value is found. \n",
    "--- \n",
    "If we were to formulate this, we would say that for N elements, we would have N + (N/2) + (N/4) + (N/8) + … 2 steps. This always turns out to be roughly 2N steps. Since Big O ignores constants, we would say that Quickselect has an efficiency of O(N).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11: Node based data structures\n",
    "- When creating an array, your code finds a contiguous group of empty cells in memory and designates them to store data for your application\n",
    "### Linked lists\n",
    "- Linked lists, on the other hand, do not consist of a bunch of memory cells in a row. Instead, they consist of a bunch of memory cells that are not next to each other, but can be spread across many different cells across the computer’s memory.\n",
    "- In addition to the data stored within the node, each node also stores the memory address of the next node in the linked list.\n",
    "- This extra piece of data—this pointer to the next node’s memory address—is known as a link.\n",
    "- Since each node contains a link to the next node, if the application is given the first node of the linked list, it can piece together the rest of the list by following the link of the first node to the second node, and the link from the second node to the third node, and so on.\n",
    "- One advantage of a linked list over an array is that the program doesn’t need to find a bunch of empty memory cells in a row to store its data.\n",
    "\n",
    "Reading:\n",
    "- All the program knows is the memory address of the first node of the linked list.\n",
    "- To find index 2 (which is the third node), the program must begin looking up index 0 of the linked list, and then follow the link at index 0 to index 1. It must then again follow the link at index 1 to index 2, and finally inspect the value at index 2.\n",
    "- If we ask a computer to look up the value at a particular index, the worst-case scenario would be if we’re looking up the last index in our list. In such a case, the computer will take N steps to look up this index, since it needs to start at the first node of the list and follow each link until it reaches the final node. Since Big O Notation expresses the worst-case scenario unless explicitly stated otherwise, we would say that reading a linked list has an efficiency of O(N). This is a significant disadvantage in comparison with arrays in which reads are O(1).\n",
    "\n",
    "Searching:\n",
    "- Arrays and linked lists have the same efficiency for search. Remember, searching is looking for a particular piece of data within the list and getting its index. With both arrays and linked lists, the program needs to start at the first cell and look through each and every cell until it finds the value it’s searching for. In a worst-case scenario—where the value we’re searching for is in the final cell or not in the list altogether—it would take O(N) steps.\n",
    "\n",
    "Insertion:\n",
    "- Insertion is one operation in which linked lists can have a distinct advantage over arrays in certain situations.\n",
    "- Recall that the worst-case scenario for insertion into an array is when the program inserts data into index 0, because it then has to shift the rest of the data one cell to the right, which ends up yielding an efficiency of O(N).\n",
    "- With linked lists, however, insertion at the beginning of the list takes just one step—which is O(1) as all that needs to be done is to update some pointers. \n",
    "- Inserting into the middle of a linked list takes O(N), as before we can update the pointers we need to search through the list to find the correct position (which takes N steps) just as it does for an array.\n",
    "- Inserting at the beginning is great for linked lists, but terrible for arrays. And inserting at the end is an array’s best-case scenario, but the worst case when it comes to a linked list.\n",
    "\n",
    "Deletion:\n",
    "- Deletion is very similar to insertion in terms of efficiency. \n",
    "- To delete a node from the beginning of a linked list, all we need to do is perform one step: we change the first_node of the linked list to now point to the second node.\n",
    "- When it comes to deleting the final node of a linked list, the actual deletion takes one step—we just take the second-to-last node and make its link null. However, it takes N steps to first get to the second-to-last node, since we need to start at the beginning of the list and follow the links until we reach it.\n",
    "\n",
    "\n",
    "### Doubly Linked Lists\n",
    "- However, if we use a special variant of a linked list called the doubly linked list, we’d be able to insert and delete data from a queue at O(1).\n",
    "- A doubly linked list is like a linked list, except that each node has two links—one that points to the next node, and one that points to the preceding node. \n",
    "- In addition, the doubly linked list keeps track of both the first and last nodes.\n",
    "- Since a doubly linked list always knows where both its first and last nodes are, we can access each of them in a single step, or O(1).\n",
    "- Because doubly linked lists have immediate access to both the front and end of the list, they can insert data on either side at O(1) as well as delete data on either side at O(1). \n",
    "- Since doubly linked lists can insert data at the end in O(1) time and delete data from the front in O(1) time, they make the perfect underlying data structure for a queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 12: Binary trees\n",
    "- A tree is also a node-based data structure.\n",
    "- The uppermost node (in our example, the “j”) is called the root\n",
    "- A binary tree is a tree that abides by the following rules: \n",
    "  - Each node has either zero, one, or two children. \n",
    "  - If a node has two children, it must have one child that has a lesser value than the parent, and one child that has a greater value than the parent.\n",
    "  \n",
    "Searching:\n",
    "- Inspect the value at the node. \n",
    "    - If we’ve found the value we’re looking for, great! \n",
    "    - If the value we’re looking for is less than the current node, search for it in its left subtree. \n",
    "    - If the value we’re looking for is greater than the current node, search for it in its right subtree.\n",
    "\n",
    "- More generally, we’d say that searching in a binary tree is O(log N). This is because each step we take eliminates half of the remaining possible values in which our value can be stored. (in a perfectly balanced binary tree, which is the best-case scenario.)\n",
    "\n",
    "Insertion:\n",
    "- Insertion always takes just one extra step beyond a search, which means that insertion takes log N + 1 steps, which is O(log N) as Big O ignores constants.\n",
    "- It emerges that in a worst-case scenario, where a tree is completely imbalanced, search is O(N). In a best-case scenario, where it is perfectly balanced, search is O(log N). In the typical scenario, in which data is inserted in random order, a tree will be pretty well balanced and search will take about O(log N).\n",
    "\n",
    "Deletion:\n",
    "- If the node being deleted has no children, simply delete it. \n",
    "- If the node being deleted has one child, delete it and plug the child into the spot where the deleted node was. \n",
    "- If the node being deleted has two children, replace the deleted node with the successor node. \n",
    "  - The successor node is the child node whose value is the least of all values that are greater than the deleted node. If the successor node has a right child, after plugging the successor node into the spot of the deleted node, take the right child of the successor node and turn it into the left child of the parent of the successor node.\n",
    "- Like search and insertion, deleting from trees is also typically O(log N)\n",
    "- The process of visiting every node in a data structure is known as traversing the data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Graphs\n",
    "- A graph is a data structure that specializes in relationships, as it easily conveys how data is connected.\n",
    "- In graph jargon, each node is called a vertex, and each line is called an edge. \n",
    "- Vertices that are connected by an edge are said to be adjacent to each other.\n",
    "- Because relationships in Twitter are one-directional, we use arrows in our visual implementation, and such a graph is known as a directed graph. \n",
    "- In Facebook, where the relationships are mutual and we use simple lines, the graph is called a non-directed graph.\n",
    "\n",
    "## Breadth-First Search\n",
    "1. We remove a vertex from the queue to designate it as the current vertex.\n",
    "2. For each current vertex, we visit each of its adjacent vertices.\n",
    "3. Now, each vertex ends up being removed from the queue once. In Big O Notation, this is called O(V). That is, for V vertices in the graph, there are V removals from the queue. Since there are O(V) removals from the queue, and O(E) visits, we say that breadth-first search has an efficiency of O(V + E).\n",
    "---\n",
    "- A weighted graph is like a regular graph but contains additional information about the edges in the graph.\n",
    "\n",
    "Dijkstra’s Algorithm:\n",
    "Here are the rules of Dijkstra’s algorithm (don’t worry—they’ll become clearer when we walk through our example): We make the starting vertex our current vertex. We check all the vertices adjacent to the current vertex and calculate and record the weights from the starting vertex to all known locations. To determine the next current vertex, we find the cheapest unvisited known vertex that can be reached from our starting vertex. Repeat the first three steps until we have visited every vertex in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 14: Space Constraints\n",
    "- There are situations,where we need to measure algorithm efficiency by another measure known as space complexity, which is how much memory an algorithm consumes.\n",
    "- Interestingly, computer scientists use Big O Notation to describe space complexity just as they do to describe time complexity.\n",
    "- If a function does not consume any memory in addition to memory used by the original inout, we’d describe the space complexity of this function as being O(1).\n",
    "- In this book, we judge the space complexity based on additional memory consumed—known as auxiliary space—meaning that we don’t count the original data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.6.5",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
